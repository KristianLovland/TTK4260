\subsection{Maksimal sannsynlighet}
Forrige avsnitt ga en rent geometrisk tolkning av minste kvadrater. Ofte har min imidlertid kunnskap om de statistiske egenskapene til støyen og feilen som forsøpler dataen din, og denne informasjonen er gjerne nyttig å bruke. Utgangspunktet for dette er sannsynlighetsfordelingen, skrevet som $p(y ; \theta)$. Ofte operer man med $\theta$ fiksert og $y$ varierende. I vårt tilfelle er $y$ gitt (den utgjør datasettet vårt $\mathcal{D}$, og vi er ute etter å finne en $\theta$ som best forklarer dette. Da kalles $p(y ; \theta)$ for \textbf{sannsynlighet}.

Med dette definert er vi klare for å definere vår maksimale sannsynlighetsestimator
\begin{equation}
\widehat{\theta}_{\mathrm{ML}}=\arg \max _{\theta \in \Theta} p(\mathcal{D} ; \theta)
\end{equation}
Vi ser at denne ligner i formen på LS-estimatoren, men at funksjonen $p$ gir oss mer valgfrihet, og muligheter til å inkludere kjent informasjon om modell og data.

Det er verdt å merke seg at et ML-estimat ikke nødvendigvis trenger å eksistere. Man kan komme med moteksempler, men under realistiske antagelser eksisterer stort sett et ML-estimat. 

Et viktig eksempel på en ML-estimator er når $p$ er normalfordelt. Da vil sannsynlighetsfunksjonen til et datasett være gitt av
\begin{equation}
p\left(y_{1}, \ldots, y_{N} ; m, \sigma^{2}\right)=\prod_{t=1}^{N} p\left(y_{t} ; m, \sigma^{2}\right)=\prod_{t=1}^{N}\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2} \frac{\left(y_{t}-m\right)^{2}}{\sigma^{2}}\right)\right)
\end{equation}
Dette grumsete uttrykket motiverer definisjonen av log-sannsynlighetsfunksjonen. Siden $\log(\cdot)$-funksjonen er monotont stigende i input-argumentet sitt, vil maksimerende input til funksjonen være lik maksimerende input til logaritmen av funksjonen. Vi definerer
\begin{equation}
\ell(\theta):=-\log p(\mathcal{D} ; \theta)
\end{equation}
I eksempelet med normalfordelt $p$ vil vi nå kunne formulere
\begin{equation}
\widehat{\theta}_{\mathrm{ML}}=\arg \max _{\theta \in \Theta} p(\mathcal{D} ; \theta) = \arg \min _{\theta \in \Theta} \ell(\theta)
\end{equation}
Eksponential- og logaritmefunksjonen spiller hverandre gode, og ved litt regning kan man se at
\begin{equation}
\arg \min _{m \in \mathbb{R}, \sigma^{2} \in \mathbb{R}_{+}} \ell\left(m, \sigma^{2}\right)=\arg \min _{m \in \mathbb{R}, \sigma^{2} \in \mathbb{R}_{+}} N \log \left(\sigma^{2}\right)+\frac{\sum_{t=1}^{N}\left(y_{t}-m\right)^{2}}{\sigma^{2}}
\end{equation}
Dennes gradient settes lik null, men her vil det inngå informasjon vi ikke har tilgang på. Vi ender opp med å benytte estimatene, og får
\begin{equation}
\bar{m}=\frac{1}{N} \sum_{t=1}^{N} y_{t}
\end{equation}
\begin{equation}
\bar{\sigma}^{2}=\frac{1}{N} \sum_{t=1}^{N}\left(y_{t}-\bar{m}\right)^{2}
\end{equation}
som ser fornuftig ut. Man kan utlede forskjellige estimatorer avhengig av $p$, men som nevnt kommer det meste av estimatorer vi jobber med til å basere seg på antagelsen om at verden er normalfordelt.
