\subsection{Minste kvadrater}
En naturlig tolkning av spørsmålet om å best mulig forklare datasettet vha $\theta$, er å finne parameteren som ved bruk av vår antatte modell $f(u_t; \theta)$, gir den korteste avstanden fra predikert datasett til faktisk datasett. "Avstand" har her den vanlige, euklidiske tolkningen, slik at en minste kvadraters estimator av en parameter $\theta$ gitt et datasett $\mathcal{D}$ og en modell $f(u_t; \theta)$, er gitt av
\begin{equation}
\widehat{\theta}_{\mathrm{LS}}=\arg \min _{\theta \in \Theta}\left\|\left[\begin{array}{c}
{y_{1}} \\
{\vdots} \\
{y_{N}}
\end{array}\right]-\left[\begin{array}{c}
{f\left(u_{1} ; \theta\right)} \\
{\vdots} \\
{f\left(u_{N} ; \theta\right)}
\end{array}\right]\right\|^{2}=\arg \min _{\theta \in \Theta} \sum_{t=1}^{N}\left(y_{t}-f\left(u_{t} ; \theta\right)\right)^{2}
\end{equation}
En nyttig verdi som følger av dette estimatet er residualen $r_{t}(\theta):=y_{t}-f\left(u_{t} ; \theta\right)$.

En nyttig klasse av problemer er de \textbf{separable} problemene. Disse er på formen
\begin{equation}
y_{t}=\sum_{j=1}^{n} \theta_{j} \phi_{j}\left(u_{t}\right)+e_{t}
\end{equation}
Dvs. at parameterne som skal estimeres inngår lineært i modellen vår. Da kan $\phi(u_t)$ være så komplisert den vil, LS-problemet vil uansett reduseres til et lineært likningssett på formen
\begin{equation}
\left[\begin{array}{c}
{y_{1}} \\
{\vdots} \\
{y_{N}}
\end{array}\right]=\left[\begin{array}{ccc}
{\phi_{1}\left(u_{1}\right)} & {\cdots} & {\phi_{n}\left(u_{1}\right)} \\
{\vdots} & {} & {\vdots} \\
{\phi_{1}\left(u_{N}\right)} & {\cdots} & {\phi_{n}\left(u_{N}\right)}
\end{array}\right]\left[\begin{array}{c}
{\theta_{1}} \\
{\vdots} \\
{\theta_{n}}
\end{array}\right]+\left[\begin{array}{c}
{e_{1}} \\
{\vdots} \\
{e_{N}}
\end{array}\right]
\end{equation}
Som mer kompakt kan skrives som
\begin{equation}
\boldsymbol{y}=\Phi(\boldsymbol{u}) \boldsymbol{\theta}+\boldsymbol{e}
\end{equation}
Målet vårt er å minimere $\mathbf{e}$. Dette kan gjøres vha. lineær programmering, men om vi ikke har begrensninger i $\theta$, dvs. $\Theta = \mathbb{R}^n$ for en eller annen $n$, kan dette løses eksplisitt som
\begin{equation}
\widehat{\theta}_{\mathrm{LS}}=\arg \min _{\boldsymbol{\theta} \in \mathbb{R}^{n}}\|\boldsymbol{y}-\Phi(\boldsymbol{u}) \boldsymbol{\theta}\|^{2}
\end{equation}
Ved å derivere dette og sette det lik null får vi \textbf{normallikningene}
\begin{equation}
\Phi(\boldsymbol{u})^{T} \Phi(\boldsymbol{u}) \widehat{\theta}_{\mathrm{LS}}=\Phi(\boldsymbol{u})^{T} \boldsymbol{y}
\end{equation}
Men hva om $
\Phi(\boldsymbol{u})^{T} \Phi(\boldsymbol{u})
$ ikke er invertibel? Da vil ikke normallikningene ha noen entydig løsning. Dette ordner vi ved å bruke \textbf{pseudoinversen}, som har noen kjekke egenskaper (nærmeste løsning hvis ingen løsning eksisterer, løsning med minst norm hvis mange løsninger eksisterer).

Noen ganger er imidlertid ikke alle tilstander like viktige. Dette løser vi ved å multiplisere hver feil med en vekt, slik at minimeringsproblemet (for ubegrensede problemer) blir
\begin{equation}
\widehat{\theta}_{\mathrm{WLS}}=\arg \min _{\boldsymbol{\theta} \in \mathbb{R}^{n}}(\boldsymbol{y}-\Phi(\boldsymbol{u}) \boldsymbol{\theta})^{T} W(\boldsymbol{y}-\Phi(\boldsymbol{u}) \boldsymbol{\theta})
\end{equation}
Mer kompakt kan dette skrives som $
=\arg \min _{\boldsymbol{\theta} \in \mathbb{R}^{n}}\|\boldsymbol{y}-\Phi(\boldsymbol{u}) \boldsymbol{\theta}\|_{W^{-1}}^{2}
$. Et nytt sett med normalligninger faller ut av dette, nå blir 
\begin{equation}
\Phi(\boldsymbol{u})^{T} W \Phi(\boldsymbol{u}) \boldsymbol{\theta}=\Phi(\boldsymbol{u})^{T} W \boldsymbol{y}
\end{equation}
som kan løses likt som tidligere, f.eks. med pseudoinvers.

Hva om problemet vårt er ulineært (ikke separabelt)? Optimeringsproblemet kan fortsatt være veldefinert, og da kan det løses numerisk. MATLAB gjør dette med \texttt{fmincon}, og de fleste programmeringsspråk med respekt for seg selv har rammeverk som gjør det samme.